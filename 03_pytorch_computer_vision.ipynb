{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 51,
   "id": "ee22df4f",
=======
   "execution_count": 2,
   "id": "d6a02bd5-03be-42c1-ab71-ab12b8c33238",
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Thu_Jun_11_22:26:48_Pacific_Daylight_Time_2020\n",
      "Cuda compilation tools, release 11.0, V11.0.194\n",
      "Build cuda_11.0_bu.relgpu_drvr445TC445_37.28540450_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "771f0f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU is available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"You have a GPU available!\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Your current GPU device is:\", device)\n",
    "else:\n",
    "    print(\"No GPU is available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6edfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\\ntorvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 3,
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
   "id": "41df0097-20a5-4043-a49a-299a04a9f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training data\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),  # images one as PIL format, we want to turn info Torch tensors\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "# Setup test data\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,  # get test data\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 4,
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
   "id": "7f016b21-07ea-4fb9-a06e-d12289bc6308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
       "           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
       "           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
       "           0.0157, 0.0000, 0.0000, 0.0118],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
       "           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0471, 0.0392, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
       "           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
       "           0.3020, 0.5098, 0.2824, 0.0588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
       "           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
       "           0.5529, 0.3451, 0.6745, 0.2588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
       "           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
       "           0.4824, 0.7686, 0.8980, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
       "           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
       "           0.8745, 0.9608, 0.6784, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
       "           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
       "           0.8627, 0.9529, 0.7922, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
       "           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
       "           0.8863, 0.7725, 0.8196, 0.2039],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
       "           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
       "           0.9608, 0.4667, 0.6549, 0.2196],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
       "           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
       "           0.8510, 0.8196, 0.3608, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
       "           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
       "           0.8549, 1.0000, 0.3020, 0.0000],\n",
       "          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
       "           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
       "           0.8784, 0.9569, 0.6235, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
       "           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
       "           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
       "           0.9137, 0.9333, 0.8431, 0.0000],\n",
       "          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
       "           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
       "           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
       "           0.8627, 0.9098, 0.9647, 0.0000],\n",
       "          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
       "           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
       "           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
       "           0.8706, 0.8941, 0.8824, 0.0000],\n",
       "          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
       "           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
       "           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
       "           0.8745, 0.8784, 0.8980, 0.1137],\n",
       "          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
       "           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
       "           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
       "           0.8627, 0.8667, 0.9020, 0.2627],\n",
       "          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
       "           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
       "           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
       "           0.7098, 0.8039, 0.8078, 0.4510],\n",
       "          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
       "           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
       "           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
       "           0.6549, 0.6941, 0.8235, 0.3608],\n",
       "          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
       "           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
       "           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
       "           0.7529, 0.8471, 0.6667, 0.0000],\n",
       "          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
       "           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
       "           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
       "           0.3882, 0.2275, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
       "           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 9)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 3,
=======
     "execution_count": 4,
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = train_data[0]\n",
    "image, label"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 5,
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
   "id": "cacb90d6-d171-4bfb-bcbe-abf9a9e21ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 7,
=======
     "execution_count": 5,
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 6,
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
   "id": "3b44b0c4-5d7d-4731-9530-94eff2b7af7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 60000, 10000, 10000)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 8,
=======
     "execution_count": 6,
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae75869-4fbf-47c1-955c-b9bf5ce4db81",
   "metadata": {},
   "source": [
    "We've got 60,000 training samples and 10,000 testing samples"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 7,
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
   "id": "3172b20e-b2fa-458c-b0e6-5e015f4c2557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
<<<<<<< HEAD
     "execution_count": 9,
=======
     "execution_count": 7,
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 8,
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
   "id": "18c939df-431a-4087-9c47-28d106202dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = train_data[1]\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "plt.imshow(image.squeeze())  # image shape is [1, 28, 28] (colour channels heigh, width)\n",
    "plt.title(label);"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 9,
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
   "id": "cfda4fcb-61a7-42a1-9fee-72543ee98722",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "plt.title(class_names[label]);"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 10,
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
   "id": "27b2e2a9-d0d7-4c60-909e-56085133426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "rows, cols = 4, 4\n",
    "for i in range(1, rows*cols + 1):\n",
    "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
    "    img, label = train_data[random_idx]\n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db30b020-723b-4844-b27e-413b84c5226f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x000001929BB5A4D0>, <torch.utils.data.dataloader.DataLoader object at 0x000001929B6233D0>)\n",
=======
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x7f57858ccd10>, <torch.utils.data.dataloader.DataLoader object at 0x7f5787506510>)\n",
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
      "Length of train dataloader: 1875 batches of 32\n",
      "Length of test dataloader: 313 batches of 32\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(train_data,  # dataset to turn into terable\n",
    "                              batch_size=BATCH_SIZE,  # how many samples per batch\n",
    "                              shuffle=True)\n",
    "test_dataloader = DataLoader(test_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=False)\n",
    "\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\")\n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 12,
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
   "id": "923a1903-5f92-4474-834a-2dcd8857f20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
      ]
     },
<<<<<<< HEAD
     "execution_count": 11,
=======
     "execution_count": 12,
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out what's inside the training dataloader\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 13,
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
   "id": "62fc0cec-3c8e-487e-b833-e1bec53d137b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([1, 28, 28])\n",
<<<<<<< HEAD
      "Label: 9, label size: torch.Size([])\n"
     ]
=======
      "Label: 6, label size: torch.Size([])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
    "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.title(class_names[label])\n",
    "plt.axis(\"Off\")\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label: {label}, label size: {label.shape}\")  # label = target value, img = input data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 14,
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
   "id": "c6205c0d-00e2-4cde-b7d8-ef85eab50f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before flattening: torch.Size([1, 28, 28]) -> [color_channels, height, width]\n",
      "Shape after flattening: torch.Size([1, 784]) -> [color_channels, height*width]\n"
     ]
    }
   ],
   "source": [
    "# Create a flatten layer\n",
    "flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n",
    "\n",
    "# Get a single sample\n",
    "x = train_features_batch[0]\n",
    "\n",
    "# Flatten the sample\n",
    "output = flatten_model(x) # perform forward pass\n",
    "\n",
    "# Print out what happened\n",
    "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b209e-6bdf-4c2c-a345-162d5071d6bf",
   "metadata": {},
   "source": [
    "Small test for running model_0 on CPU vs. a similar model on GPU soon."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 15,
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
   "id": "0289eb74-6519-4a2a-b69e-e89be751196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
<<<<<<< HEAD
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
=======
    "            nn.Flatten(),  # neural networks like their inputs  in vector form\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_unit),\n",
    "            nn.Linear(in_features=hidden_unit, out_features=output_shape)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0678f4b-0d8c-4d9e-a00b-2107326a042b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV0(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model_0 = FashionMNISTModelV0(input_shape=784,  # one for every pixel\n",
    "                              hidden_unit=10,  # how many units in the hidden layer\n",
    "                              output_shape=len(class_names)  # one for every class\n",
    "                             )\n",
    "model_0.to(\"cpu\")  # keep model on CPU to begin with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "032a4aa7-7bd4-4839-95b5-dfe27df7442f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading helper functions.py\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "    print(\"helper_function.py already exists, skipping download\")\n",
    "else:\n",
    "    print(\"Downloading helper functions.py\")\n",
    "    request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "    with open(\"helper_function.py\", \"wb\") as f:\n",
    "        f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbcfc5d2-ec3a-4830-a094-2501c5bde343",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e190115b-9aa6-4953-8771-a1ff11f0f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()  # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct  / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f7032e8-9512-44ed-b4ec-1ad3a6791d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dcb7be2-8b0b-4038-95b2-a85e11440897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d33ee8659f422b813e989e225f035f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "-----\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.59039 | Test loss: 0.50954, Test acc: 82.04%\n",
      "\n",
      "Epoch: 1\n",
      "-----\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.47633 | Test loss: 0.47989, Test acc: 83.20%\n",
      "\n",
      "Epoch: 2\n",
      "-----\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.45503 | Test loss: 0.47664, Test acc: 83.43%\n",
      "\n",
      "Train time on cpu: 23.332 seconds\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in  tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-----\")\n",
    "    ### Training\n",
    "    train_loss = 0\n",
    "    # Add a loop to loop through training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_0.train()\n",
    "        # 1. Forward pass\n",
    "        y_pred = model_0(X)\n",
    "\n",
    "        # 2. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss  # accumulatively add up the loss per epoch\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print out how many samples have been seen\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    ### Testing\n",
    "    # Setup variables for accumulatively adding up loss and accuracy\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y, in test_dataloader:\n",
    "            # 1. Forward pass\n",
    "            test_pred = model_0(X)\n",
    "            # 2. Calculate loss (accumulatively)\n",
    "            test_loss += loss_fn(test_pred, y)  # accumulatively add up the loss per epoch\n",
    "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "\n",
    "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        # Divide total accuracy by length of test dataloader (per batch)\n",
    "        test_acc /= len(test_dataloader)\n",
    "    \n",
    "    ## Print out what's happening\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
    "\n",
    "# Calculate training time\n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n",
    "                                           end=train_time_end_on_cpu,\n",
    "                                           device=str(next(model_0.parameters()).device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75d6868-9639-42dd-a817-99b3646cfffe",
   "metadata": {},
   "source": [
    "###  Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eac78420-fb1c-4d83-8557-8a60f68a5189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTModelV0',\n",
       " 'model_loss': 0.47663894295692444,\n",
       " 'model_acc': 83.42651757188499}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y, in data_loader:\n",
    "            # Make predicitons with the model\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y,\n",
    "                               y_pred=y_pred.argmax(dim=1))  # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "\n",
    "    return {\"model_name\": model.__class__.__name__,  # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Calcualte model 0 results on test dataset\n",
    "model_0_results = eval_model(model=model_0,\n",
    "                             data_loader=test_dataloader,\n",
    "                             loss_fn=loss_fn,\n",
    "                             accuracy_fn=accuracy_fn)\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f2c2b-2b9a-4163-a85d-916321e586d0",
   "metadata": {},
   "source": [
    "## Setup device agnostic-code (for using a GPU if there is one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a83d238c-04cf-4159-b149-d27a9a372b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966536a2-fe3a-4723-adfe-92297c0ae901",
   "metadata": {},
   "source": [
    "## Model 1: Building a better model with non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98992d17-91f9-4b03-a2be-8474dddabdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a model with non-linear and linear layers\n",
    "class FashionMNISTModelV1(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), # flatten inputs into single vector\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f76fc677-fcb0-4bed-9006-1ddc00a65023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model_1 = FashionMNISTModelV1(input_shape=784, # number of input features\n",
    "    hidden_units=10,\n",
    "    output_shape=len(class_names) # number of output classes desired\n",
    ").to(device) # send model to GPU if it's available\n",
    "next(model_1.parameters()).device # check model device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eede983e-afa9-4698-95e5-79479c5e549a",
   "metadata": {},
   "source": [
    "### Setup loss, optimizer and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3dc549bd-013f-42c8-8201-ffafa4bb9228",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), \n",
    "                            lr=0.01)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()  # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f41c1c-ef1f-4b44-94f3-6b0edf387738",
   "metadata": {},
   "source": [
    "### Functionizing training and test loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dede1b35-eb83-400a-a46f-141125b13056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    model.to(device)\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # Send data to GPU\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy_fn(y_true=y,\n",
    "                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate loss and accuracy per epoch and print out what's happening\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "def test_step(data_loader: torch.utils.data.DataLoader,\n",
    "              model: torch.nn.Module,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy_fn,\n",
    "              device: torch.device = device):\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.to(device)\n",
    "    model.eval() # put model in eval mode\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode(): \n",
    "        for X, y in data_loader:\n",
    "            # Send data to GPU\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X)\n",
    "            \n",
    "            # 2. Calculate loss and accuracy\n",
    "            test_loss += loss_fn(test_pred, y)\n",
    "            test_acc += accuracy_fn(y_true=y,\n",
    "                y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
    "            )\n",
    "        \n",
    "        # Adjust metrics and print out\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2000ce01-ffba-4e8b-b189-8321d602b1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b82b4d7b0914c908c8d65df4eb1cd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------\n",
      "Train loss: 0.57637 | Train accuracy: 78.65%\n",
      "Test loss: 0.62411 | Test accuracy: 77.13%\n",
      "\n",
      "Epoch 1\n",
      "-------\n",
      "Train loss: 0.57244 | Train accuracy: 78.85%\n",
      "Test loss: 0.62252 | Test accuracy: 77.02%\n",
      "\n",
      "Epoch 2\n",
      "-------\n",
      "Train loss: 0.57057 | Train accuracy: 78.95%\n",
      "Test loss: 0.62067 | Test accuracy: 77.28%\n",
      "\n",
      "Train time on cuda: 24.087 seconds\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Measure time\n",
    "from timeit import default_timer as timer\n",
    "train_time_start_on_gpu = timer()\n",
    "\n",
    "epochs = 3\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch {epoch}\\n-------\")\n",
    "    train_step(data_loader=train_dataloader, \n",
    "        model=model_1, \n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )\n",
    "    \n",
    "    test_step(data_loader=test_dataloader,\n",
    "    model=model_1,\n",
    "    loss_fn=loss_fn,\n",
    "    accuracy_fn=accuracy_fn)\n",
    "\n",
    "train_time_end_on_gpu = timer()\n",
    "total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n",
    "                                            end=train_time_end_on_gpu,\n",
    "                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f78867aa-7992-4901-9f1d-b8817c4dd244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)\n",
    "\n",
    "# # This will error due to `eval_model()` not using device agnostic code\n",
    "# model_1_resu;ts = eval_model(model=model_1,\n",
    "#                              data_loader=test_dataloader,\n",
    "#                              loss_fn=loss_fn,\n",
    "#                              accuracy_fn=accuracy_fn)\n",
    "# model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3433e5cc-2fc8-4624-bc0c-c31112b44efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTModelV1',\n",
       " 'model_loss': 0.6206665635108948,\n",
       " 'model_acc': 77.27635782747603}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "    \n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y, in data_loader:\n",
    "            # Send data to the target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
    "\n",
    "        # Scale loss and acc\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "\n",
    "    return {\"model_name\": model.__class__.__name__,   # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Calculate model 1 results with device-agnostic code\n",
    "model_1_results = eval_model(model=model_1, data_loader=test_dataloader,\n",
    "                             loss_fn=loss_fn, accuracy_fn=accuracy_fn, device=device)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d0f2dccb-745b-4315-b69d-75c74a6deecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTModelV0',\n",
       " 'model_loss': 0.47663894295692444,\n",
       " 'model_acc': 83.42651757188499}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8defbb0b-c21d-46cb-9e37-0d5dadf1a772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV2(\n",
       "  (block_1): Sequential(\n",
       "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block_2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FashionMNISTModelV2(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.block_2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2)  # default stride value is same as kernel_size\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Where did this in_features shape come from?\n",
    "            # It's because each layer of our network compress and changes the shape of our inputs data\n",
    "            nn.Linear(in_features=hidden_units*7*7, out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.block_1(x)\n",
    "        # print (x.shape)\n",
    "        x = self.block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model_2 = FashionMNISTModelV2(input_shape=1,\n",
    "                              hidden_units=10,\n",
    "                              output_shape=len(class_names)).to(device)\n",
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5468f934-6751-4a80-9849-bb7c64ffa388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([32, 3, 64, 64])\n",
      "Single image shape: torch.Size([3, 64, 64])\n",
      "Single image pixel values:\n",
      " tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  1.8446, -1.1845,  1.3835],\n",
      "         [ 1.4451,  0.8564,  2.2181,  ...,  0.3399,  0.7200,  0.4114],\n",
      "         [ 1.9312,  1.0119, -1.4364,  ..., -0.5558,  0.7043,  0.7099],\n",
      "         ...,\n",
      "         [-0.5610, -0.4830,  0.4770,  ..., -0.2713, -0.9537, -0.6737],\n",
      "         [ 0.3076, -0.1277,  0.0366,  ..., -2.0060,  0.2824, -0.8111],\n",
      "         [-1.5486,  0.0485, -0.7712,  ..., -0.1403,  0.9416, -0.0118]],\n",
      "\n",
      "        [[-0.5197,  1.8524,  1.8365,  ...,  0.8935, -1.5114, -0.8515],\n",
      "         [ 2.0818,  1.0677, -1.4277,  ...,  1.6612, -2.6223, -0.4319],\n",
      "         [-0.1010, -0.4388, -1.9775,  ...,  0.2106,  0.2536, -0.7318],\n",
      "         ...,\n",
      "         [ 0.2779,  0.7342, -0.3736,  ..., -0.4601,  0.1815,  0.1850],\n",
      "         [ 0.7205, -0.2833,  0.0937,  ..., -0.1002, -2.3609,  2.2465],\n",
      "         [-1.3242, -0.1973,  0.2920,  ...,  0.5409,  0.6940,  1.8563]],\n",
      "\n",
      "        [[-0.7978,  1.0261,  1.1465,  ...,  1.2134,  0.9354, -0.0780],\n",
      "         [-1.4647, -1.9571,  0.1017,  ..., -1.9986, -0.7409,  0.7011],\n",
      "         [-1.3938,  0.8466, -1.7191,  ..., -1.1867,  0.1320,  0.3407],\n",
      "         ...,\n",
      "         [ 0.8206, -0.3745,  1.2499,  ..., -0.0676,  0.0385,  0.6335],\n",
      "         [-0.5589, -0.3393,  0.2347,  ...,  2.1181,  2.4569,  1.3083],\n",
      "         [-0.4092,  1.5199,  0.2401,  ..., -0.2558,  0.7870,  0.9924]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Create sample batch of random numbers with same size as image batch\n",
    "images = torch.randn(size=(32, 3, 64, 64))  # [batch size, color_channels, height, width]\n",
    "test_image = images[0]  # get a single image for testing\n",
    "print(f\"Image batch shape: {images.shape}\")\n",
    "print(f\"Single image shape: {test_image.shape}\")\n",
    "print(f\"Single image pixel values:\\n {test_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "371ff3c2-0943-4b92-ba6e-1e15c3434c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 62, 62])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a convolutional layer with same dimensions as TinyVGG\n",
    "# Try changing any of the parameters and see waht happens\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "conv_layer(test_image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4a5b1888-6273-4439-90c1-0fd5da427cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 64, 64])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add extra dimension to test image\n",
    "test_image.unsqueeze(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eb3bab2e-a802-41eb-ab2e-b01e59d2b127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 62, 62])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass test image with extra dimension through conv_layer\n",
    "conv_layer(test_image.unsqueeze(dim=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6f029e72-8819-4b7e-9064-30252be19734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 30, 30])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Create a new conv_layer with different values (try setting these to whatever you like)\n",
    "conv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image\n",
    "                         out_channels=10,\n",
    "                         kernel_size=(5, 5), # kernel is usually a square so a tuple also works\n",
    "                         stride=2,\n",
    "                         padding=0)\n",
    "\n",
    "# Pass single image through new conv_layer_2 (this calls nn.Conv2d()'s forward() method on the input)\n",
    "conv_layer_2(test_image.unsqueeze(dim=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "36ddd62a-6484-458f-85a6-4c99b11f5553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 30, 30])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer_2(test_image.unsqueeze(dim=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "809b3a59-af20-4ce8-bfa6-8d180dfd8afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[[[ 0.1471,  0.1597, -0.0451],\n",
      "          [ 0.1768, -0.0422,  0.0388],\n",
      "          [-0.0937,  0.1130,  0.1697]],\n",
      "\n",
      "         [[-0.1412,  0.1673,  0.0360],\n",
      "          [ 0.1422,  0.0261,  0.0928],\n",
      "          [-0.0272,  0.1484,  0.0284]],\n",
      "\n",
      "         [[-0.0898,  0.0491, -0.0887],\n",
      "          [-0.0226, -0.0782,  0.1277],\n",
      "          [-0.1519, -0.0887, -0.0543]]],\n",
      "\n",
      "\n",
      "        [[[-0.1157,  0.0182, -0.1901],\n",
      "          [ 0.1738, -0.1635,  0.1486],\n",
      "          [ 0.0320, -0.0625,  0.1189]],\n",
      "\n",
      "         [[ 0.0300,  0.1555,  0.0210],\n",
      "          [-0.0607,  0.0517, -0.0522],\n",
      "          [ 0.0810,  0.1718,  0.1112]],\n",
      "\n",
      "         [[-0.0841,  0.1111,  0.0344],\n",
      "          [ 0.0977, -0.1173, -0.1905],\n",
      "          [-0.0744, -0.1476,  0.1579]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0554,  0.0797,  0.0609],\n",
      "          [-0.0033,  0.1506, -0.1367],\n",
      "          [ 0.0121, -0.1314,  0.0593]],\n",
      "\n",
      "         [[-0.0663,  0.0590, -0.0401],\n",
      "          [ 0.1596, -0.1141, -0.1148],\n",
      "          [-0.1148,  0.1731,  0.0641]],\n",
      "\n",
      "         [[ 0.1852, -0.1588, -0.1909],\n",
      "          [-0.1506, -0.1295,  0.0780],\n",
      "          [ 0.0689,  0.1599, -0.0994]]],\n",
      "\n",
      "\n",
      "        [[[-0.1312,  0.1021, -0.0778],\n",
      "          [ 0.1168, -0.0457,  0.1101],\n",
      "          [-0.1495, -0.0971,  0.0587]],\n",
      "\n",
      "         [[ 0.0407, -0.0491,  0.1147],\n",
      "          [ 0.1308, -0.1396, -0.1027],\n",
      "          [ 0.1762, -0.0649, -0.0682]],\n",
      "\n",
      "         [[-0.1862, -0.1102,  0.0481],\n",
      "          [-0.0254, -0.1397,  0.0045],\n",
      "          [-0.1315, -0.1633, -0.1060]]],\n",
      "\n",
      "\n",
      "        [[[-0.1684, -0.1225,  0.1924],\n",
      "          [ 0.0363,  0.0593, -0.1795],\n",
      "          [-0.1264, -0.0641,  0.0301]],\n",
      "\n",
      "         [[-0.1693, -0.0829, -0.1152],\n",
      "          [ 0.0005, -0.0716, -0.0133],\n",
      "          [-0.1304, -0.1321, -0.1123]],\n",
      "\n",
      "         [[-0.0659, -0.1519,  0.1614],\n",
      "          [-0.0382,  0.1656,  0.0600],\n",
      "          [-0.1630,  0.1332, -0.0530]]],\n",
      "\n",
      "\n",
      "        [[[-0.0738, -0.1597, -0.1913],\n",
      "          [ 0.0551, -0.0420,  0.0749],\n",
      "          [-0.1579,  0.1429, -0.1413]],\n",
      "\n",
      "         [[-0.0332,  0.0402,  0.0994],\n",
      "          [ 0.1554,  0.1753, -0.1526],\n",
      "          [ 0.0484, -0.0828, -0.0211]],\n",
      "\n",
      "         [[-0.1440,  0.1753, -0.1412],\n",
      "          [ 0.1029,  0.0676,  0.0625],\n",
      "          [-0.1040,  0.1749,  0.0423]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0248, -0.1696,  0.0808],\n",
      "          [-0.0289, -0.0882,  0.1653],\n",
      "          [ 0.0429, -0.1065, -0.0974]],\n",
      "\n",
      "         [[-0.0092,  0.1075, -0.0492],\n",
      "          [-0.1098, -0.0659, -0.1438],\n",
      "          [ 0.0686,  0.1490, -0.1812]],\n",
      "\n",
      "         [[ 0.0447,  0.0994,  0.0349],\n",
      "          [-0.0685,  0.1004,  0.1011],\n",
      "          [ 0.0720, -0.0338, -0.0510]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0206, -0.0340, -0.0574],\n",
      "          [ 0.1230,  0.1654, -0.0191],\n",
      "          [-0.0431,  0.0028, -0.0115]],\n",
      "\n",
      "         [[ 0.0463,  0.0539, -0.1748],\n",
      "          [-0.0710,  0.1621,  0.0750],\n",
      "          [-0.0096, -0.1160, -0.1177]],\n",
      "\n",
      "         [[-0.1724, -0.0627,  0.0650],\n",
      "          [ 0.1227,  0.0889, -0.1701],\n",
      "          [-0.1157, -0.0304,  0.1862]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0278, -0.0498,  0.0796],\n",
      "          [-0.0733, -0.1246,  0.1405],\n",
      "          [-0.0875, -0.0386, -0.1915]],\n",
      "\n",
      "         [[ 0.1288,  0.1458,  0.0701],\n",
      "          [-0.1342, -0.1899, -0.1563],\n",
      "          [ 0.1435,  0.0924,  0.1619]],\n",
      "\n",
      "         [[ 0.1008,  0.0487, -0.0019],\n",
      "          [-0.1464, -0.1649, -0.1800],\n",
      "          [ 0.0788, -0.0945, -0.0387]]],\n",
      "\n",
      "\n",
      "        [[[-0.1108, -0.0351, -0.1355],\n",
      "          [-0.1257,  0.0638, -0.0572],\n",
      "          [ 0.1188, -0.0617, -0.1412]],\n",
      "\n",
      "         [[-0.0340, -0.0933, -0.0589],\n",
      "          [-0.1832,  0.1077, -0.1340],\n",
      "          [ 0.0967,  0.0873,  0.1375]],\n",
      "\n",
      "         [[-0.1476,  0.1384, -0.0910],\n",
      "          [ 0.0714,  0.1807, -0.0271],\n",
      "          [-0.0015, -0.0443, -0.1607]]]])), ('bias', tensor([ 0.0924, -0.1910,  0.1195,  0.1440,  0.1820, -0.0454, -0.1581,  0.0433,\n",
      "         0.1063, -0.1915]))])\n"
     ]
    }
   ],
   "source": [
    "print(conv_layer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "95f8e757-f278-45ab-85e1-90759ec81595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_layer_2 weight shape: \n",
      "torch.Size([10, 3, 5, 5]) -> [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\n",
      "\n",
      "conv_layer_2 bias shape: \n",
      "torch.Size([10]) -> [out_channels=10]\n"
     ]
    }
   ],
   "source": [
    "# Get shapes of weight and bias tensors within conv_layer_2\n",
    "print(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -> [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\")\n",
    "print(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -> [out_channels=10]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618fbb14-cadb-4ba2-9680-9d93bb95a501",
   "metadata": {},
   "source": [
    "### Check out what happens whe we move data through nn.MaxPool2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e79a41-6ada-433d-8720-e29b310d1483",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test image original shape: {test_image.shape}\")\n",
    "print(f\"Test image with unsqueeze dimension: {test_image.unsqueeze(dim=00.shape}\")\n",
    "\n",
    "# Create a sample nn.MaxPool2d() layer\n",
    "max_pool_layer = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "# Pass "
>>>>>>> 22ba257cfa401e134f3ba721ab47645d4a2f4e92
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e09f50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV0(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model_0 = FashionMNISTModelV0(input_shape=784,\n",
    "                            hidden_units=10,\n",
    "                            output_shape=len(class_names)\n",
    "                            )\n",
    "model_0.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72655170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a635b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    total_time = end - start\n",
    "    print(f\"Train on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9494eb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ecad7058ea465a87bd6a3f7786e211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.45517 | Test loss: 0.47248, Test acc: 83.40%\n",
      "\n",
      "Epoch: 1\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.44442 | Test loss: 0.46817, Test acc: 83.75%\n",
      "\n",
      "Epoch: 2\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.43614 | Test loss: 0.46825, Test acc: 83.74%\n",
      "\n",
      "Train on cpu: 32.616 seconds\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    ### Training\n",
    "    train_loss = 0\n",
    "    # Add a loop to loop through training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_0.train()\n",
    "        # 1. Forward pass\n",
    "        y_pred = model_0(X)\n",
    "        \n",
    "        # 2. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss # accumulatively add up the loss per epoch\n",
    "        \n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print out how many samples have been seen\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    ### Testing\n",
    "    # Setup variables for accumulatively adding up loss and accuracy \n",
    "    test_loss, test_acc = 0, 0 \n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            # 1. Forward pass\n",
    "            test_pred = model_0(X)\n",
    "           \n",
    "            # 2. Calculate loss (accumatively)\n",
    "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
    "\n",
    "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "        \n",
    "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        # Divide total accuracy by length of test dataloader (per batch)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    ## Print out what's happening\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
    "\n",
    "# Calculate training time      \n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n",
    "                                           end=train_time_end_on_cpu,\n",
    "                                           device=str(next(model_0.parameters()).device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcac48f4",
   "metadata": {},
   "source": [
    "### Predictions and Moldel 0 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1821982c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTModelV0',\n",
       " 'model_loss': 0.46825188398361206,\n",
       " 'model_acc': 83.73602236421725}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Make predictions with the model\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "           \"model_loss\": loss.item(),\n",
    "           \"model_acc\": acc}\n",
    "\n",
    "# Calculate model 0 results on test dataset\n",
    "model_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n",
    "                            loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
    "                            )\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a29e942",
   "metadata": {},
   "source": [
    "### Setup device agnostic code (for using a GPU if there is one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61edb1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/cu110/torch_stable.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following yanked versions: 2.0.0\n",
      "ERROR: Could not find a version that satisfies the requirement torchaudio==0.10.0+cu110 (from versions: 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2)\n",
      "ERROR: No matching distribution found for torchaudio==0.10.0+cu110\n"
     ]
    }
   ],
   "source": [
    "!pip install torchaudio==0.10.0+cu110 torchvision==0.11.1+cu110 -f https://download.pytorch.org/whl/cu110/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1b49a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f9540fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5279e0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Thu_Jun_11_22:26:48_Pacific_Daylight_Time_2020\n",
      "Cuda compilation tools, release 11.0, V11.0.194\n",
      "Build cuda_11.0_bu.relgpu_drvr445TC445_37.28540450_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3bdc1e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torchaudio\n",
      "Version: 2.1.2\n",
      "Summary: An audio package for PyTorch\n",
      "Home-page: https://github.com/pytorch/audio\n",
      "Author: Soumith Chintala, David Pollack, Sean Naren, Peter Goldsborough, Moto Hira, Caroline Chen, Jeff Hwang, Zhaoheng Ni, Xiaohui Zhang\n",
      "Author-email: soumith@pytorch.org\n",
      "License: \n",
      "Location: C:\\Users\\kajte\\anaconda3\\Lib\\site-packages\n",
      "Requires: torch\n",
      "Required-by: \n",
      "---\n",
      "Name: torchvision\n",
      "Version: 0.16.2\n",
      "Summary: image and video datasets and models for torch deep learning\n",
      "Home-page: https://github.com/pytorch/vision\n",
      "Author: PyTorch Core Team\n",
      "Author-email: soumith@pytorch.org\n",
      "License: BSD\n",
      "Location: C:\\Users\\kajte\\anaconda3\\Lib\\site-packages\n",
      "Requires: numpy, pillow, requests, torch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075883ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
