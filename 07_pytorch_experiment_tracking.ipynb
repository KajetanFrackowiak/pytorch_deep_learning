{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44c46cec-7f8a-4cf4-a8b2-6e5dcf54883c",
   "metadata": {},
   "source": [
    "## 0. Getting setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c92707b0-1341-44d1-911d-302614158dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.2.2\n",
      "torchvision version: 0.17.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")\n",
    "\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Coudnl't find torchinfo... installing it\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "try:\n",
    "    from going_modular import data_setup, engine\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find going_modular scripts...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d9c7242-3f89-4d4a-bc48-6e235d1d56d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8227e79e-6f6e-42a7-b8ce-f83256ecb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed: int=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ad0c67-d005-464d-9607-c058665b4a29",
   "metadata": {},
   "source": [
    "# 1. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "437cec2b-c9b8-41e3-a912-1cea3e600942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data/pizza_steak_sushi directory exists, skipping download.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('data/pizza_steak_sushi')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "def download_data(source: str, \n",
    "                  destination: str,\n",
    "                  remove_source: bool = True) -> Path:\n",
    "    # Setup path to data folder\n",
    "    data_path = Path(\"data/\")\n",
    "    image_path = data_path / destination\n",
    "\n",
    "    # If the image folder doesn't exist, download it and prepare it... \n",
    "    if image_path.is_dir():\n",
    "        print(f\"[INFO] {image_path} directory exists, skipping download.\")\n",
    "    else:\n",
    "        print(f\"[INFO] Did not find {image_path} directory, creating one...\")\n",
    "        image_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Download pizza, steak, sushi data\n",
    "        target_file = Path(source).name\n",
    "        with open(data_path / target_file, \"wb\") as f:\n",
    "            request = requests.get(source)\n",
    "            print(f\"[INFO] Downloading {target_file} from {source}...\")\n",
    "            f.write(request.content)\n",
    "\n",
    "        # Unzip pizza, steak, sushi data\n",
    "        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n",
    "            print(f\"[INFO] Unzipping {target_file} data...\") \n",
    "            zip_ref.extractall(image_path)\n",
    "\n",
    "        # Remove .zip file\n",
    "        if remove_source:\n",
    "            os.remove(data_path / target_file)\n",
    "    \n",
    "    return image_path\n",
    "\n",
    "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                           destination=\"pizza_steak_sushi\")\n",
    "image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767b40c-6e55-4ad9-b4da-c89931c6ab4f",
   "metadata": {},
   "source": [
    "# 2. Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a9886549-5d88-4b05-b7d0-1b97747184a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually created transforms: Compose(\n",
      "    Resize(size=(224, 224), interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.226])\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7ef76d19e410>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7ef76cef5250>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.226])\n",
    "\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize])\n",
    "print(f\"Manually created transforms: {manual_transforms}\")\n",
    "\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=manual_transforms,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36d4acdb-ca65-4c1d-88f6-fa66969ba73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # for torchvision < 13\n",
    "# weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT for torchvision > 0.13\n",
    "# model = torchvision.models.efficientnet_b0(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f9bb4272-ce88-4ab3-b157-844491a4b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all base layers\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "set_seeds()\n",
    "\n",
    "model.classfier = torch.nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280,\n",
    "              out_features=len(class_names),\n",
    "              bias=True).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "17b9671c-029f-4997-bbc4-50ba90046006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "\n",
    "# summary(model,\n",
    "#         input_size=(32, 3, 224, 224),\n",
    "#         verbose=0,\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"]\n",
    "#        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee14d08-234e-4bd4-82f5-6824db3ed403",
   "metadata": {},
   "source": [
    "# 4. Train model and track results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "00c1650f-7d65-4f4c-aa2c-a6b7f1f623f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fba57228-b69d-4148-b760-13a1dc95f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import wandb\n",
    "except:\n",
    "    !pip install wandb\n",
    "    import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "296f66e0-0632-4baf-96a5-4b4e0ecb277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from tqdm.auto import tqdm\n",
    "from going_modular.engine import train_step, test_step\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          criterion: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device,\n",
    "         wandb_project_name: str = None,\n",
    "         wandb_run_name: str = None) -> Dict[str, List]:\n",
    "    \n",
    "    if wandb_project_name is not None:\n",
    "        wandb.init(project=wandb_project_name, name=wandb_project_name)\n",
    "    \n",
    "    results = {\"train_loss\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_acc\": []\n",
    "              }\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=test_dataloader,\n",
    "                                           loss_fn=criterion,\n",
    "                                           optimizer=optimizer,\n",
    "                                           device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                         dataloader=test_dataloader,\n",
    "                                         loss_fn=criterion,\n",
    "                                         device=device)\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f} | \"\n",
    "        )\n",
    "\n",
    "        if wandb_project_name is not None:\n",
    "            wandb.log({\"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc})\n",
    "            \n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4efd42d9-a7bc-46ce-9cff-3d941334ef36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrackowiak\u001b[0m (\u001b[33mfrackowiak_kajetan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kajetan/Documents/pytorch_deep_learning/wandb/run-20240407_184312-t9g885g8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frackowiak_kajetan/torch_experiment_track/runs/t9g885g8' target=\"_blank\">torch_experiment_track</a></strong> to <a href='https://wandb.ai/frackowiak_kajetan/torch_experiment_track' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frackowiak_kajetan/torch_experiment_track' target=\"_blank\">https://wandb.ai/frackowiak_kajetan/torch_experiment_track</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frackowiak_kajetan/torch_experiment_track/runs/t9g885g8' target=\"_blank\">https://wandb.ai/frackowiak_kajetan/torch_experiment_track/runs/t9g885g8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd0599225a54729bcaaa537d21bb5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 7.6200 | test_loss: 8.8285 | train_acc: 0.0000 | test_acc: 0.0000 | \n",
      "Epoch: 2 | train_loss: 6.5586 | test_loss: 7.3306 | train_acc: 0.0000 | test_acc: 0.0000 | \n",
      "Epoch: 3 | train_loss: 5.8622 | test_loss: 5.8943 | train_acc: 0.0000 | test_acc: 0.0000 | \n",
      "Epoch: 4 | train_loss: 4.9484 | test_loss: 4.7190 | train_acc: 0.1023 | test_acc: 0.0720 | \n",
      "Epoch: 5 | train_loss: 4.1468 | test_loss: 3.8594 | train_acc: 0.1544 | test_acc: 0.1544 | \n",
      "Epoch: 6 | train_loss: 3.5626 | test_loss: 3.2013 | train_acc: 0.1544 | test_acc: 0.1752 | \n",
      "Epoch: 7 | train_loss: 2.9876 | test_loss: 2.7624 | train_acc: 0.2879 | test_acc: 0.2576 | \n",
      "Epoch: 8 | train_loss: 2.5725 | test_loss: 2.4692 | train_acc: 0.3797 | test_acc: 0.3087 | \n",
      "Epoch: 9 | train_loss: 2.2193 | test_loss: 2.2241 | train_acc: 0.4318 | test_acc: 0.3400 | \n",
      "Epoch: 10 | train_loss: 2.0125 | test_loss: 2.0421 | train_acc: 0.4318 | test_acc: 0.4621 | \n",
      "Epoch: 11 | train_loss: 1.8522 | test_loss: 1.8956 | train_acc: 0.4299 | test_acc: 0.5028 | \n",
      "Epoch: 12 | train_loss: 1.7278 | test_loss: 1.8166 | train_acc: 0.5028 | test_acc: 0.5540 | \n",
      "Epoch: 13 | train_loss: 1.8032 | test_loss: 1.7481 | train_acc: 0.5133 | test_acc: 0.5852 | \n",
      "Epoch: 14 | train_loss: 1.5664 | test_loss: 1.6980 | train_acc: 0.5436 | test_acc: 0.6061 | \n",
      "Epoch: 15 | train_loss: 1.4215 | test_loss: 1.6213 | train_acc: 0.5540 | test_acc: 0.6165 | \n",
      "Epoch: 16 | train_loss: 1.4251 | test_loss: 1.6029 | train_acc: 0.6061 | test_acc: 0.6269 | \n",
      "Epoch: 17 | train_loss: 1.3933 | test_loss: 1.5545 | train_acc: 0.5445 | test_acc: 0.6477 | \n",
      "Epoch: 18 | train_loss: 1.2070 | test_loss: 1.4951 | train_acc: 0.6458 | test_acc: 0.6686 | \n",
      "Epoch: 19 | train_loss: 1.2406 | test_loss: 1.4381 | train_acc: 0.5653 | test_acc: 0.6686 | \n",
      "Epoch: 20 | train_loss: 1.1957 | test_loss: 1.3968 | train_acc: 0.5464 | test_acc: 0.6487 | \n"
     ]
    }
   ],
   "source": [
    "set_seeds()\n",
    "model0 = train(model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                optimizer=optimizer,\n",
    "                criterion=criterion,\n",
    "                epochs=20,\n",
    "                device=device,\n",
    "               wandb_project_name=\"torch_experiment_track\",\n",
    "               wandb_run_name=\"torch_experiment_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f999ef38-b2d3-4686-aad6-163ad425bc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data/pizza_steak_sushi directory exists, skipping download.\n",
      "[INFO] data/pizza_steak_sushi_20_percent directory exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "data_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                                     destination=\"pizza_steak_sushi\")\n",
    "\n",
    "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "                                     destination=\"pizza_steak_sushi_20_percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "51e13267-7f0d-4c27-ad6a-c53e43f95706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_directory 10%: data/pizza_steak_sushi/train\n",
      "Training_directory 20%: data/pizza_steak_sushi_20_percent/train\n",
      "Testing_directory data/pizza_steak_sushi/test\n"
     ]
    }
   ],
   "source": [
    "train_dir_10_percent = data_10_percent_path / \"train\"\n",
    "train_dir_20_percent = data_20_percent_path / \"train\"\n",
    "\n",
    "test_dir = data_10_percent_path / \"test\"\n",
    "\n",
    "print(f\"Training_directory 10%: {train_dir_10_percent}\")\n",
    "print(f\"Training_directory 20%: {train_dir_20_percent}\")\n",
    "print(f\"Testing_directory {test_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b8e9b656-62bd-4a92-91bd-03123893bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b688ccb2-116f-4f05-ad5d-99edb7953532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches of size 32 in 10 percent training data: 8\n",
      "Number of batches of size 32 in 20 percent training data: 15\n",
      "Number of batches of size 32 in testing data: 3 (all experiments will use the same test data)\n",
      "Number of classes 3, class_names ['pizza', 'steak', 'sushi']\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir_10_percent,\n",
    "    test_dir=test_dir,\n",
    "    transform=simple_transform,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "train_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir_20_percent,\n",
    "    test_dir=test_dir,\n",
    "    transform=simple_transform,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(test_dataloader)} (all experiments will use the same test data)\")\n",
    "print(f\"Number of classes {len(class_names)}, class_names {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97d85667-5ecf-41dc-ad16-55f3b117ce6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kajetan/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/kajetan/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B2_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of in_features to final layer of EfficientNetB2; 1408\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchinfo import summary\n",
    "\n",
    "effnetb2 = torchvision.models.efficientnet_b2(pretrained=True).to(device)\n",
    "\n",
    "# summary(model=effnetb2,\n",
    "#         input_size=(32, 3, 224, 224),\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"]\n",
    "# )\n",
    "\n",
    "print(f\"Number of in_features to final layer of EfficientNetB2; {len(effnetb2.classifier.state_dict()['1.weight'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6e904cb0-cd4b-46df-b188-6002b63a5bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "OUT_FEATURES = len(class_names)\n",
    "\n",
    "def create_effnetb0():\n",
    "    model = torchvision.models.efficientnet_b0(pretrained=True).to(device)\n",
    "\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    set_seeds()\n",
    "\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(in_features=1280, out_features=OUT_FEATURES)\n",
    "    ).to(device)\n",
    "\n",
    "    model.name = \"effnetb0\"\n",
    "    print(f\"[INFO] Created new {model.name} model.\")\n",
    "    return model\n",
    "\n",
    "def create_effnetb2():\n",
    "    model = torchvision.models.efficientnet_b2(pretrained=True).to(device)\n",
    "\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    set_seeds()\n",
    "\n",
    "    model.classfier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3),\n",
    "        nn.Linear(in_features=1408, out_features=OUT_FEATURES)\n",
    "    ).to(device)\n",
    "\n",
    "    model.name = \"effnetb2\"\n",
    "    print(f\"[INFO] Created new {model.name} model.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eab32e82-c9ea-4a78-a988-7cc0cecaed0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created new effnetb0 model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kajetan/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "effnetb0 = create_effnetb0()\n",
    "\n",
    "# summary(model=effnetb0,\n",
    "#         input_size=(32, 3, 224, 224),\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"]\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ed7e7302-7375-49af-b521-ef8c8e75e406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created new effnetb2 model.\n"
     ]
    }
   ],
   "source": [
    "effnetb2 = create_effnetb2()\n",
    "\n",
    "# summary(model=effnetb2,\n",
    "#         input_size=(32, 3, 224, 224),\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"]\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7943c6a3-db5c-4915-a789-e203c9c4dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = [5, 10]\n",
    "models = [\"effnetb0\", \"effnetb2\"]\n",
    "train_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,\n",
    "                     \"data_20_percent\": train_dataloader_20_percent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "73326153-398f-4825-a40f-5f45ad0c4dd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from going_modular.utils import save_model\n",
    "\n",
    "# set_seeds(seed=42)\n",
    "\n",
    "# experiment_number = 0\n",
    "\n",
    "# for dataloader_name, train_dataloader in train_dataloaders.items():\n",
    "#     for epochs in num_epochs:\n",
    "#         for model_name in models:\n",
    "#             experiment_number += 1\n",
    "#             print(f\"[INFO] Experiment number: {experiment_number}\")\n",
    "#             print(f\"[INFO] Model: {model_name}\")\n",
    "#             print(f\"[INFO] DataLoader: {dataloader_name}\")\n",
    "#             print(f\"[INFO] Number of epochs: {epochs}\")\n",
    "\n",
    "#             if model_name == \"effnetb0\":\n",
    "#                 model = create_effnetb0()\n",
    "#             else:\n",
    "#                 model = create_effnetb2()\n",
    "\n",
    "#             criterion = nn.CrossEntropyLoss()\n",
    "#             optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "\n",
    "#             train(model=model,\n",
    "#                 train_dataloader=test_dataloader,\n",
    "#                 test_dataloader=test_dataloader,\n",
    "#                 optimizer=optimizer,\n",
    "#                 criterion=criterion,\n",
    "#                 epochs=epochs,\n",
    "#                 device=device,\n",
    "#                 wandb_project_name=model_name,\n",
    "#                 wandb_run_name=model_name)\n",
    "\n",
    "#             save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"\n",
    "#             save_model(model=model,\n",
    "#                         target_dir=\"models\",\n",
    "#                         model_name=save_filepath)\n",
    "#             print(\"-\"*50+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6446c961-422a-4a97-b57e-6452690dff73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:t9g885g8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_acc</td><td>▁▁▁▂▃▃▄▄▅▆▆▇▇▇▇█████</td></tr><tr><td>test_loss</td><td>█▇▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▃▃▄▅▆▆▆▆▇▇▇█▇█▇▇</td></tr><tr><td>train_loss</td><td>█▇▆▅▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_acc</td><td>0.64867</td></tr><tr><td>test_loss</td><td>1.39679</td></tr><tr><td>train_acc</td><td>0.5464</td></tr><tr><td>train_loss</td><td>1.19573</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">torch_experiment_track</strong> at: <a href='https://wandb.ai/frackowiak_kajetan/torch_experiment_track/runs/t9g885g8' target=\"_blank\">https://wandb.ai/frackowiak_kajetan/torch_experiment_track/runs/t9g885g8</a><br/> View job at <a href='https://wandb.ai/frackowiak_kajetan/torch_experiment_track/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE1OTA3Mjk0MA==/version_details/v0' target=\"_blank\">https://wandb.ai/frackowiak_kajetan/torch_experiment_track/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE1OTA3Mjk0MA==/version_details/v0</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240407_184312-t9g885g8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:t9g885g8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5446d1784d234c4d92327a84085713a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112666111118111, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kajetan/Documents/pytorch_deep_learning/wandb/run-20240407_184421-oy1pok76</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frackowiak_kajetan/effnetb0v1/runs/oy1pok76' target=\"_blank\">effnetb0v1</a></strong> to <a href='https://wandb.ai/frackowiak_kajetan/effnetb0v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frackowiak_kajetan/effnetb0v1' target=\"_blank\">https://wandb.ai/frackowiak_kajetan/effnetb0v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frackowiak_kajetan/effnetb0v1/runs/oy1pok76' target=\"_blank\">https://wandb.ai/frackowiak_kajetan/effnetb0v1/runs/oy1pok76</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68551dc327a14d9a8e5e93441c43b5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 2.4503 | test_loss: 0.8755 | train_acc: 0.0625 | test_acc: 0.6013 | \n",
      "Epoch: 2 | train_loss: 0.5073 | test_loss: 0.8259 | train_acc: 0.8551 | test_acc: 0.6250 | \n",
      "Epoch: 3 | train_loss: 1.1038 | test_loss: 0.6423 | train_acc: 0.6250 | test_acc: 0.6979 | \n",
      "Epoch: 4 | train_loss: 0.9805 | test_loss: 0.3882 | train_acc: 0.6884 | test_acc: 0.8655 | \n",
      "Epoch: 5 | train_loss: 0.7653 | test_loss: 0.4030 | train_acc: 0.5388 | test_acc: 0.8883 | \n",
      "Epoch: 6 | train_loss: 0.5729 | test_loss: 0.2458 | train_acc: 0.7339 | test_acc: 1.0000 | \n",
      "Epoch: 7 | train_loss: 0.3167 | test_loss: 0.2238 | train_acc: 0.9479 | test_acc: 0.9271 | \n",
      "Epoch: 8 | train_loss: 0.3818 | test_loss: 0.2105 | train_acc: 0.7708 | test_acc: 0.9167 | \n",
      "Epoch: 9 | train_loss: 0.4800 | test_loss: 0.1466 | train_acc: 0.7812 | test_acc: 1.0000 | \n",
      "Epoch: 10 | train_loss: 0.3921 | test_loss: 0.1499 | train_acc: 0.8854 | test_acc: 1.0000 | \n",
      "Epoch: 11 | train_loss: 0.3328 | test_loss: 0.1357 | train_acc: 0.9375 | test_acc: 1.0000 | \n",
      "Epoch: 12 | train_loss: 0.2735 | test_loss: 0.1121 | train_acc: 0.9375 | test_acc: 1.0000 | \n",
      "Epoch: 13 | train_loss: 0.2396 | test_loss: 0.1042 | train_acc: 0.9479 | test_acc: 1.0000 | \n",
      "Epoch: 14 | train_loss: 0.2565 | test_loss: 0.0945 | train_acc: 0.9167 | test_acc: 1.0000 | \n",
      "Epoch: 15 | train_loss: 0.2506 | test_loss: 0.0920 | train_acc: 0.9375 | test_acc: 1.0000 | \n",
      "Epoch: 16 | train_loss: 0.2506 | test_loss: 0.0896 | train_acc: 0.9384 | test_acc: 1.0000 | \n",
      "Epoch: 17 | train_loss: 0.2277 | test_loss: 0.0789 | train_acc: 0.9688 | test_acc: 1.0000 | \n",
      "Epoch: 18 | train_loss: 0.1435 | test_loss: 0.0740 | train_acc: 0.9896 | test_acc: 1.0000 | \n",
      "Epoch: 19 | train_loss: 0.1709 | test_loss: 0.0696 | train_acc: 0.9688 | test_acc: 1.0000 | \n",
      "Epoch: 20 | train_loss: 0.1551 | test_loss: 0.0665 | train_acc: 0.9792 | test_acc: 1.0000 | \n",
      "Epoch: 21 | train_loss: 0.1581 | test_loss: 0.0640 | train_acc: 1.0000 | test_acc: 1.0000 | \n",
      "Epoch: 22 | train_loss: 0.1555 | test_loss: 0.0602 | train_acc: 0.9896 | test_acc: 1.0000 | \n",
      "Epoch: 23 | train_loss: 0.1229 | test_loss: 0.0556 | train_acc: 0.9896 | test_acc: 1.0000 | \n",
      "Epoch: 24 | train_loss: 0.1260 | test_loss: 0.0541 | train_acc: 1.0000 | test_acc: 1.0000 | \n",
      "Epoch: 25 | train_loss: 0.1556 | test_loss: 0.0522 | train_acc: 1.0000 | test_acc: 1.0000 | \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [2.4502642154693604,\n",
       "  0.5072961747646332,\n",
       "  1.1038346538941066,\n",
       "  0.9804738412300745,\n",
       "  0.765303244193395,\n",
       "  0.5729138255119324,\n",
       "  0.3166517565647761,\n",
       "  0.3818257947762807,\n",
       "  0.4800354490677516,\n",
       "  0.3920569171508153,\n",
       "  0.33276456594467163,\n",
       "  0.2734677344560623,\n",
       "  0.23961861928304037,\n",
       "  0.25652217864990234,\n",
       "  0.25063203523556393,\n",
       "  0.2505962550640106,\n",
       "  0.22769281268119812,\n",
       "  0.14345781256755194,\n",
       "  0.17087763672073683,\n",
       "  0.15508605043093363,\n",
       "  0.1581234261393547,\n",
       "  0.155532938738664,\n",
       "  0.12287664910157521,\n",
       "  0.1260321040948232,\n",
       "  0.1556467500825723],\n",
       " 'test_loss': [0.8755178252855936,\n",
       "  0.8259320010741552,\n",
       "  0.6422924523552259,\n",
       "  0.3882146974404653,\n",
       "  0.4030019889275233,\n",
       "  0.2458317130804062,\n",
       "  0.22384149581193924,\n",
       "  0.21052697052558264,\n",
       "  0.14659111822644869,\n",
       "  0.14989589899778366,\n",
       "  0.13571483890215555,\n",
       "  0.1121266521513462,\n",
       "  0.1041627787053585,\n",
       "  0.09454988191525142,\n",
       "  0.09199534108241399,\n",
       "  0.08959916730721791,\n",
       "  0.07888854170838992,\n",
       "  0.07399249883989494,\n",
       "  0.06961978288988273,\n",
       "  0.06652151358624299,\n",
       "  0.064038406436642,\n",
       "  0.060210137317577996,\n",
       "  0.05562405474483967,\n",
       "  0.05405356797079245,\n",
       "  0.052244758854309716],\n",
       " 'train_acc': [0.0625,\n",
       "  0.8551136363636364,\n",
       "  0.625,\n",
       "  0.6884469696969697,\n",
       "  0.5388257575757576,\n",
       "  0.7339015151515151,\n",
       "  0.9479166666666666,\n",
       "  0.7708333333333334,\n",
       "  0.78125,\n",
       "  0.8854166666666666,\n",
       "  0.9375,\n",
       "  0.9375,\n",
       "  0.9479166666666666,\n",
       "  0.9166666666666666,\n",
       "  0.9375,\n",
       "  0.9384469696969697,\n",
       "  0.96875,\n",
       "  0.9895833333333334,\n",
       "  0.96875,\n",
       "  0.9791666666666666,\n",
       "  1.0,\n",
       "  0.9895833333333334,\n",
       "  0.9895833333333334,\n",
       "  1.0,\n",
       "  1.0],\n",
       " 'test_acc': [0.6013257575757576,\n",
       "  0.625,\n",
       "  0.6979166666666666,\n",
       "  0.8655303030303031,\n",
       "  0.8882575757575758,\n",
       "  1.0,\n",
       "  0.9270833333333334,\n",
       "  0.9166666666666666,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0]}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model=effnetb0,\n",
    "      train_dataloader=train_dataloader,\n",
    "      test_dataloader=test_dataloader,\n",
    "      optimizer=torch.optim.Adam(params=effnetb0.parameters(), lr=1e-2),\n",
    "      criterion=criterion,\n",
    "      epochs=25,\n",
    "      device=device,\n",
    "      wandb_project_name=\"effnetb0v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "86a85814-c933-4015-bbe6-62550358edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "\n",
    "    if checkpoint['model_name'] == \"effnetb0V1\":\n",
    "        model = create_effnetb0()  # This line creates the model architecture\n",
    "    elif checkpoint['model_name'] == \"effnetb0V2\":\n",
    "        model = create_effnetb2()  # This line creates the model architecture\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model name\")\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])  # This line loads the trained parameters\n",
    "\n",
    "    print(f\"[INFO] Model '{checkpoint['model_name']}' loaded successfully\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "09f028c5-7ca2-4139-98ca-690c15419343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "\n",
    "    if checkpoint['model_name'] == \"effnetb0V1\":\n",
    "        model = create_effnetb0()\n",
    "    elif checkpoint['model_name'] == \"effnetb0V2\":\n",
    "        model = create_effnetb2()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model name\")\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    print(f\"[INFO] Model '{checkpoint['model_name']}' loaded successfully\")\n",
    "    return model\n",
    "\n",
    "# effnetV1_model_path = \"models/effnetb0V1.pth\"\n",
    "# best_model = load_model(effnetV1_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "398ab7a5-5093-43d4-bd33-cacb6aaef8ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/bonurke.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check the model file size\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 4\u001b[0m effnetb0_model_size \u001b[38;5;241m=\u001b[39m Path(effnetV1_model_path)\u001b[38;5;241m.\u001b[39mstat()\u001b[38;5;241m.\u001b[39mst_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1024\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEffnetb0V1_loaded feature extractor model size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meffnetb0_model_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/pathlib.py:1013\u001b[0m, in \u001b[0;36mPath.stat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstat\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, follow_symlinks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;124;03m    Return the result of the stat() system call on this path, like\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;124;03m    os.stat() does.\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1013\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mstat(\u001b[38;5;28mself\u001b[39m, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/bonurke.pth'"
     ]
    }
   ],
   "source": [
    "# Check the model file size\n",
    "from pathlib import Path\n",
    "\n",
    "effnetb0_model_size = Path(effnetV1_model_path).stat().st_size // (1024*1024)\n",
    "print(f\"Effnetb0V1_loaded feature extractor model size: {effnetb0_model_size} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3be56c-5b2e-41d3-887d-fe3ba67bdcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from going_modular.predictions import pred_and_plot_image\n",
    "\n",
    "# # Get a random list of 3 images from 20% test set\n",
    "# import random\n",
    "# num_images_to_plot = 3\n",
    "# test_image_path_list = list(Path(data_20_percent_path / \"test\").glob(\"*/*.jpg\")) # get all test image paths from 20% dataset\n",
    "# test_image_path_sample = random.sample(population=test_image_path_list,\n",
    "#                                        k=num_images_to_plot) # randomly select k number of images\n",
    "\n",
    "# # Iterate through random test image paths, make predictions on them and plot them\n",
    "# for image_path in test_image_path_sample:\n",
    "#     pred_and_plot_image(model=effnetV1_loaded,\n",
    "#                         image_path=image_path,\n",
    "#                         class_names=class_names,\n",
    "#                         image_size=(224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c54b3-2f6e-4b3b-b783-f09a567560cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download custom image\n",
    "# import requests\n",
    "\n",
    "# # Setup custom image path\n",
    "# custom_image_path = Path(\"data/04-pizza-dad.jpeg\")\n",
    "\n",
    "# # Download the image if it doesn't already exist\n",
    "# if not custom_image_path.is_file():\n",
    "#     with open(custom_image_path, \"wb\") as f:\n",
    "#         # When downloading from GitHub, need to use the \"raw\" file link\n",
    "#         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n",
    "#         print(f\"Downloading {custom_image_path}...\")\n",
    "#         f.write(request.content)\n",
    "# else:\n",
    "#     print(f\"{custom_image_path} already exists, skipping download.\")\n",
    "\n",
    "# # Predict on custom image\n",
    "# pred_and_plot_image(model=effnetV1_loaded,\n",
    "#                     image_path=custom_image_path,\n",
    "#                     class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e712b2bb-2361-486c-9718-f672b7b60bda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hello Kajetan",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
