{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19fae3ae",
   "metadata": {
    "id": "19fae3ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cu121\n",
      "0.17.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193d9ba8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43963,
     "status": "ok",
     "timestamp": 1712901863059,
     "user": {
      "displayName": "Kajetan FrÄ…ckowiak",
      "userId": "06064176921065523082"
     },
     "user_tz": -120
    },
    "id": "193d9ba8",
    "outputId": "8b51d875-9d3b-4d1b-ae6f-7f4725cbdfa8"
   },
   "outputs": [],
   "source": [
    "# Continue with regular imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Try to get torchinfo, install it if it doesn't work\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
    "try:\n",
    "    from going_modular import data_setup, engine, utils\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "except:\n",
    "    # Get the going_modular scripts\n",
    "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
    "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
    "    !mv pytorch-deep-learning/going_modular .\n",
    "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
    "    !rm -rf pytorch-deep-learning\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "328964cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1712901863059,
     "user": {
      "displayName": "Kajetan FrÄ…ckowiak",
      "userId": "06064176921065523082"
     },
     "user_tz": -120
    },
    "id": "328964cb",
    "outputId": "29eef5d2-4c34-447c-fc2a-ac3cd2cc0fc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fea98b",
   "metadata": {
    "id": "e6fea98b"
   },
   "source": [
    "## 1. Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "523f8929",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2089,
     "status": "ok",
     "timestamp": 1712901865144,
     "user": {
      "displayName": "Kajetan FrÄ…ckowiak",
      "userId": "06064176921065523082"
     },
     "user_tz": -120
    },
    "id": "523f8929",
    "outputId": "60d0184d-2d2d-422c-8f55-794fb4c07ab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data\\pizza_steak_sushi_20_percent directory exists, skipping download.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('data/pizza_steak_sushi_20_percent')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "                                     destination=\"pizza_steak_sushi_20_percent\")\n",
    "\n",
    "data_20_percent_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbb8449d",
   "metadata": {
    "id": "cbb8449d"
   },
   "outputs": [],
   "source": [
    "train_dir = data_20_percent_path / \"train\"\n",
    "test_dir = data_20_percent_path / \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970fe03",
   "metadata": {
    "id": "3970fe03"
   },
   "source": [
    "## 2. FoodVision Mini model deployment experiment outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a0f094",
   "metadata": {
    "id": "e5a0f094"
   },
   "source": [
    "## 3. Creating an EffNetB2 feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b1ac281",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1252,
     "status": "ok",
     "timestamp": 1712901866393,
     "user": {
      "displayName": "Kajetan FrÄ…ckowiak",
      "userId": "06064176921065523082"
     },
     "user_tz": -120
    },
    "id": "6b1ac281",
    "outputId": "9d302bb6-7e12-48c1-afaa-f2dbd58ad09f"
   },
   "outputs": [],
   "source": [
    "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "\n",
    "# 2. Get EffNetB2 transforms\n",
    "effnetb2_transforms = effnetb2_weights.transforms()\n",
    "\n",
    "# 3. Setup pretrained model\n",
    "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"\n",
    "\n",
    "# 4. Freeze the base layers in the model (this will freeze all layers to begin with)\n",
    "for param in effnetb2.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb261fac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1712901866394,
     "user": {
      "displayName": "Kajetan FrÄ…ckowiak",
      "userId": "06064176921065523082"
     },
     "user_tz": -120
    },
    "id": "eb261fac",
    "outputId": "64063f58-68ce-44a6-b041-8650012ab5fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.3, inplace=True)\n",
       "  (1): Linear(in_features=1408, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnetb2.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a72ddc-1d8b-4d7c-954f-cda635b05975",
   "metadata": {
    "id": "57a72ddc-1d8b-4d7c-954f-cda635b05975"
   },
   "outputs": [],
   "source": [
    "effnetb2.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True), # keep dropout layer same\n",
    "    nn.Linear(in_features=1408, # keep in_features same\n",
    "              out_features=3)) # change out_features to suit our number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "821a95e6-6693-46c5-876a-2a0e9a8ec6b9",
   "metadata": {
    "id": "821a95e6-6693-46c5-876a-2a0e9a8ec6b9"
   },
   "outputs": [],
   "source": [
    "def create_effnetb2_model(num_classes:int=3,\n",
    "                          seed:int=42):\n",
    "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of classes in the classifier head.\n",
    "            Defaults to 3.\n",
    "        seed (int, optional): random seed value. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): EffNetB2 feature extractor model.\n",
    "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
    "    \"\"\"\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "    # 4. Freeze all layers in base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 5. Change classifier head with random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes),\n",
    "    )\n",
    "\n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "297bc0cc-92b8-4374-befa-71004b1faf5d",
   "metadata": {
    "id": "297bc0cc-92b8-4374-befa-71004b1faf5d"
   },
   "outputs": [],
   "source": [
    "effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,\n",
    "                                                      seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f094280-41aa-4498-9d56-9cd2c3017569",
   "metadata": {
    "id": "3f094280-41aa-4498-9d56-9cd2c3017569"
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# summary(effnetb2,\n",
    "#         input_size=(1, 3, 224, 224),\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81fe2012-c495-4de6-bbe6-56fa9d663db7",
   "metadata": {
    "id": "81fe2012-c495-4de6-bbe6-56fa9d663db7"
   },
   "outputs": [],
   "source": [
    "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                 test_dir=test_dir,\n",
    "                                                                                                 transform=effnetb2_transforms,\n",
    "                                                                                                 batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ea9942e-3f24-45ee-8b4d-33e54459b412",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229,
     "referenced_widgets": [
      "d0fc3957ebc0450ca3b3ea0a8d5e3441",
      "fdecda765a884f39a3829b82cca3709e",
      "6d779ab7f66646f9b096b2ee3d69a540",
      "40fb9a6bf6f94c5fbc2234c3455f7d60",
      "f478c6f185544f41a40d600b6274a06b",
      "b30a67d3802d475b83eec5347bb5d7df",
      "a77174417993483e884f233ab66e08a6",
      "50ae02cf5a364174919949e4334033bd",
      "2e8e3b8a7ef2449ba6f2d18d257c0ffb",
      "326523d6008940a4876c4fb33fcf5a24",
      "0d57e5693e2340b19dd6835f3a14625f"
     ]
    },
    "id": "2ea9942e-3f24-45ee-8b4d-33e54459b412",
    "outputId": "da46e058-2206-4c9c-9273-a92703298fe5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb2d9dc49e44b2aad9763f5e4a551b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9797 | train_acc: 0.5687 | test_loss: 0.7365 | test_acc: 0.9347\n",
      "Epoch: 2 | train_loss: 0.7102 | train_acc: 0.8562 | test_loss: 0.5842 | test_acc: 0.9472\n",
      "Epoch: 3 | train_loss: 0.5892 | train_acc: 0.8896 | test_loss: 0.4863 | test_acc: 0.9437\n",
      "Epoch: 4 | train_loss: 0.4494 | train_acc: 0.9083 | test_loss: 0.4323 | test_acc: 0.9381\n",
      "Epoch: 5 | train_loss: 0.4303 | train_acc: 0.9000 | test_loss: 0.3882 | test_acc: 0.9290\n",
      "Epoch: 6 | train_loss: 0.4393 | train_acc: 0.8812 | test_loss: 0.3467 | test_acc: 0.9688\n",
      "Epoch: 7 | train_loss: 0.4316 | train_acc: 0.8750 | test_loss: 0.3226 | test_acc: 0.9625\n",
      "Epoch: 8 | train_loss: 0.3899 | train_acc: 0.8979 | test_loss: 0.3420 | test_acc: 0.9227\n",
      "Epoch: 9 | train_loss: 0.3734 | train_acc: 0.8979 | test_loss: 0.3100 | test_acc: 0.9256\n",
      "Epoch: 10 | train_loss: 0.3648 | train_acc: 0.8646 | test_loss: 0.2782 | test_acc: 0.9625\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
    "                             lr=1e-3)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "set_seeds()\n",
    "effnetb2.to(device)\n",
    "effnetb2_results = engine.train(model=effnetb2,\n",
    "                                train_dataloader=train_dataloader_effnetb2,\n",
    "                                test_dataloader=test_dataloader_effnetb2,\n",
    "                                epochs=10,\n",
    "                                optimizer=optimizer,\n",
    "                                loss_fn=loss_fn,\n",
    "                                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd931d-6ffd-4bef-b519-ed9ad1b62315",
   "metadata": {
    "id": "41cd931d-6ffd-4bef-b519-ed9ad1b62315"
   },
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(effnetb2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac1b3e5-dc3a-43aa-afca-4acf3c11b0e1",
   "metadata": {
    "id": "5ac1b3e5-dc3a-43aa-afca-4acf3c11b0e1"
   },
   "outputs": [],
   "source": [
    "# from going_modular import utils\n",
    "\n",
    "utils.save_model(model=effnetb2,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872dccde-3149-4e74-8b98-fc530f75fbaf",
   "metadata": {
    "id": "872dccde-3149-4e74-8b98-fc530f75fbaf"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "pretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024)\n",
    "effnetb2_loaded_results = torch.load(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")\n",
    "print(f\"Pretrained EffNetb2 feature extractor model size: {pretrained_effnetb2_model_size} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ce973-b244-4dfc-bdc8-e66ff3a7151c",
   "metadata": {
    "id": "054ce973-b244-4dfc-bdc8-e66ff3a7151c"
   },
   "outputs": [],
   "source": [
    "effnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\n",
    "effnetb2_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c91f18a-4fbe-450c-a034-8d63c7ce7c51",
   "metadata": {
    "id": "4c91f18a-4fbe-450c-a034-8d63c7ce7c51"
   },
   "outputs": [],
   "source": [
    "effnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],\n",
    "                  \"test_acc\": effnetb2_results[\"test_acc\"][-1],\n",
    "                  \"number_of_parameters\": effnetb2_total_params,\n",
    "                  \"model_size (MB)\": pretrained_effnetb2_model_size}\n",
    "effnetb2_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de959590-3378-48b7-b12b-03755f9c04cc",
   "metadata": {
    "id": "de959590-3378-48b7-b12b-03755f9c04cc"
   },
   "outputs": [],
   "source": [
    "vit = torchvision.models.vit_b_16()\n",
    "vit.heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54344b4-5155-4132-a61e-351e787b5c4b",
   "metadata": {
    "id": "e54344b4-5155-4132-a61e-351e787b5c4b"
   },
   "outputs": [],
   "source": [
    "def create_vit_model(num_classes:int=3,\n",
    "                     seed:int=42):\n",
    "    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.vit_b_16(weights=weights)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    model.heads = nn.Sequential(nn.Linear(in_features=768,\n",
    "                                        out_features=num_classes))\n",
    "\n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b3da05-adfa-4dcc-848e-2983b18e18e6",
   "metadata": {
    "id": "89b3da05-adfa-4dcc-848e-2983b18e18e6"
   },
   "outputs": [],
   "source": [
    "vit, vit_transforms = create_vit_model(num_classes=3,\n",
    "                                       seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df21820a-25e0-4eff-8783-4c1db89a9228",
   "metadata": {
    "id": "df21820a-25e0-4eff-8783-4c1db89a9228"
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# summary(vit,\n",
    "#         input_size=(1, 3, 224, 224),\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287cce0-e808-4ad5-bbd3-45242f045cf4",
   "metadata": {
    "id": "f287cce0-e808-4ad5-bbd3-45242f045cf4"
   },
   "outputs": [],
   "source": [
    "train_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                                       test_dir=test_dir,\n",
    "                                                                                       transform=vit_transforms,\n",
    "                                                                                       batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19b9966-d4b5-4c42-a22e-74afbf2c059d",
   "metadata": {
    "id": "e19b9966-d4b5-4c42-a22e-74afbf2c059d"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
    "                             lr=1e-3)\n",
    "criterium = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "set_seeds()\n",
    "vit_results = engine.train(model=vit.to(device),\n",
    "                           train_dataloader=train_dataloader_vit,\n",
    "                           test_dataloader=test_dataloader_vit,\n",
    "                           epochs=10,\n",
    "                           optimizer=optimizer,\n",
    "                           loss_fn=criterium,\n",
    "                           device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb9e4e-6486-4203-87ab-606915e7824e",
   "metadata": {
    "id": "bcfb9e4e-6486-4203-87ab-606915e7824e"
   },
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(vit_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed0c5df-3949-4363-9007-98b199f1c403",
   "metadata": {
    "id": "0ed0c5df-3949-4363-9007-98b199f1c403"
   },
   "outputs": [],
   "source": [
    "utils.save_model(model=vit,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17f23fa-75b6-44e2-99d3-e1df2f71ccce",
   "metadata": {
    "id": "c17f23fa-75b6-44e2-99d3-e1df2f71ccce"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "pretrained_vit_model_size = Path(\"models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024)\n",
    "print(f\"Pretrained vit model size: {pretrained_vit_model_size} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d7483c-d8ff-4cd2-a42c-a92118f620bc",
   "metadata": {
    "id": "52d7483c-d8ff-4cd2-a42c-a92118f620bc"
   },
   "outputs": [],
   "source": [
    "vit_total_params = sum(torch.numel(param) for param in vit.parameters())\n",
    "vit_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd141e5-e5dd-43e7-8f47-01be449200d7",
   "metadata": {
    "id": "cbd141e5-e5dd-43e7-8f47-01be449200d7"
   },
   "outputs": [],
   "source": [
    "vit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],\n",
    "            \"test_acc\": vit_results[\"test_acc\"][-1],\n",
    "            \"number_of_parameters\": vit_total_params,\n",
    "            \"model_size (MB)\": pretrained_vit_model_size}\n",
    "vit_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b250394-9f73-46f2-873b-5dca054ef562",
   "metadata": {
    "id": "9b250394-9f73-46f2-873b-5dca054ef562"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(f\"[INFO] Finding all filepaths ending with '.jpg' in directory: {test_dir}\")\n",
    "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
    "test_data_paths[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77789292-4c55-4b96-ad28-94d82eeac34f",
   "metadata": {
    "id": "77789292-4c55-4b96-ad28-94d82eeac34f"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict\n",
    "\n",
    "def pred_and_store(paths: List[pathlib.Path],\n",
    "                   model: torch.nn.Module,\n",
    "                   transform: torchvision.transforms,\n",
    "                   class_names: List[str],\n",
    "                   device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -> List[Dict]:\n",
    "    pred_list = []\n",
    "\n",
    "    for path in tqdm(paths):\n",
    "        pred_dict = {}\n",
    "\n",
    "        pred_dict[\"image_path\"] = str(path)\n",
    "        class_name = path.parent.stem\n",
    "        pred_dict[\"class_name\"] = class_name\n",
    "\n",
    "        start_time = timer()\n",
    "        img = Image.open(path)\n",
    "        transformed_image = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_logit = model(transformed_image)  # inference on target sample\n",
    "            pred_prob = torch.softmax(pred_logit, dim=1)\n",
    "            pred_label = torch.argmax(pred_prob, dim=1)\n",
    "            pred_class = class_names[pred_label.cpu()]\n",
    "\n",
    "            pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n",
    "            pred_dict[\"pred_class\"] = pred_class\n",
    "\n",
    "            end_time = timer()\n",
    "            pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)\n",
    "\n",
    "        pred_dict[\"correct\"] = class_name == pred_class\n",
    "        pred_list.append(pred_dict)\n",
    "\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b4a63-f9a5-45d2-8239-c210ddcf8cd2",
   "metadata": {
    "id": "021b4a63-f9a5-45d2-8239-c210ddcf8cd2"
   },
   "outputs": [],
   "source": [
    "effnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
    "                                          model=effnetb2,\n",
    "                                          transform=effnetb2_transforms,\n",
    "                                          class_names=class_names,\n",
    "                                          device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593a37db-10de-4fed-80d8-cf0851bb4165",
   "metadata": {
    "id": "593a37db-10de-4fed-80d8-cf0851bb4165",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "effnetb2_test_pred_dicts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba7c17-e181-4b5e-8ef7-b2aa618f3fc6",
   "metadata": {
    "id": "73ba7c17-e181-4b5e-8ef7-b2aa618f3fc6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "effnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\n",
    "effnetb2_test_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b822c54-ddc2-4c79-9624-44ad473226d3",
   "metadata": {
    "id": "3b822c54-ddc2-4c79-9624-44ad473226d3"
   },
   "outputs": [],
   "source": [
    "effnetb2_test_pred_df.correct.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71187fe2-5093-4524-9b45-0b16f23af402",
   "metadata": {
    "id": "71187fe2-5093-4524-9b45-0b16f23af402"
   },
   "outputs": [],
   "source": [
    "effnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4)\n",
    "print(f\"EffnetB2 average time per prediction: {effnetb2_average_time_per_pred} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc9a83c-3b96-4500-9332-24edf1da3721",
   "metadata": {
    "id": "9bc9a83c-3b96-4500-9332-24edf1da3721"
   },
   "outputs": [],
   "source": [
    "effnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred\n",
    "effnetb2_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c7a62-57fe-44f5-bd73-327aca424623",
   "metadata": {
    "id": "e60c7a62-57fe-44f5-bd73-327aca424623"
   },
   "outputs": [],
   "source": [
    "vit_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
    "                                     model=vit,\n",
    "                                     transform=vit_transforms,\n",
    "                                     class_names=class_names,\n",
    "                                     device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c05b7a-bdbf-4de0-95b5-b4dae7320163",
   "metadata": {
    "id": "81c05b7a-bdbf-4de0-95b5-b4dae7320163"
   },
   "outputs": [],
   "source": [
    "vit_test_pred_dicts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39909ba-9195-4fdf-82b6-fc21847e2e52",
   "metadata": {
    "id": "d39909ba-9195-4fdf-82b6-fc21847e2e52"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "vit_test_pred_df = pd.DataFrame(vit_test_pred_dicts)\n",
    "vit_test_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856eee2-afa7-4b75-a71f-1b2d2229159f",
   "metadata": {
    "id": "d856eee2-afa7-4b75-a71f-1b2d2229159f"
   },
   "outputs": [],
   "source": [
    "vit_test_pred_df.correct.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703750c4-38cc-4b9d-888d-a13f238ebddc",
   "metadata": {
    "id": "703750c4-38cc-4b9d-888d-a13f238ebddc"
   },
   "outputs": [],
   "source": [
    "vit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4)\n",
    "print(f\"Average time per pred ViT: {vit_average_time_per_pred} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e6450-6387-4350-a451-a906f4c11f78",
   "metadata": {
    "id": "0e8e6450-6387-4350-a451-a906f4c11f78"
   },
   "outputs": [],
   "source": [
    "vit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred\n",
    "vit_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa1650-08ca-4c6a-88d1-6b093a8bd0db",
   "metadata": {
    "id": "76aa1650-08ca-4c6a-88d1-6b093a8bd0db"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([effnetb2_stats, vit_stats])\n",
    "df[\"model\"] = [\"EffnetB2\", \"ViT\"]\n",
    "df[\"test_acc\"] = round(df[\"test_acc\"] * 100, 2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a168399-564e-4cf1-998a-8b4a208dcaf6",
   "metadata": {
    "id": "6a168399-564e-4cf1-998a-8b4a208dcaf6"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data=(df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffnetB2\"]),\n",
    "             columns=[\"ViT to EffNetB2 ratios\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6cff91-0787-470f-a93b-b62648972bf4",
   "metadata": {
    "id": "1e6cff91-0787-470f-a93b-b62648972bf4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab31b43c-59f6-401b-a13f-3d83fbfe4aa5",
   "metadata": {
    "id": "ab31b43c-59f6-401b-a13f-3d83fbfe4aa5"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.scatter(data=df,\n",
    "                     x=\"time_per_pred_cpu\",\n",
    "                     y=\"test_acc\",\n",
    "                     c=[\"blue\", \"orange\"],\n",
    "                     s=df[\"model_size (MB)\"])  # Use model sizes as dot sizes\n",
    "ax.set_title(\"FoodVision Mini Inference Speed vs Performance\", fontsize=18)\n",
    "ax.set_xlabel(\"Prediction time per image (seconds)\", fontsize=14)\n",
    "ax.set_ylabel(\"Test accuracy (%)\", fontsize=14)\n",
    "ax.grid(True)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    ax.annotate(text=row[\"model\"],\n",
    "                xy=(row[\"time_per_pred_cpu\"]+0.0006, row[\"test_acc\"]+0.03),\n",
    "                fontsize=12)\n",
    "\n",
    "handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.5)\n",
    "model_size_legend = ax.legend(handles,\n",
    "                              labels,\n",
    "                              loc=\"lower right\",\n",
    "                              title=\"Model size (MB)\",\n",
    "                              fontsize=12)\n",
    "plt.savefig(\"images/09-foodvision-mini-inference-speed-vs-performance.jpg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc0f85-2722-4db2-81d0-1fd2385d64c8",
   "metadata": {
    "id": "9acc0f85-2722-4db2-81d0-1fd2385d64c8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import gradio as gr\n",
    "except:\n",
    "    !pip -q install gradio\n",
    "    import gradio as gr\n",
    "print(f\"Gradio version: {gr.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3645f6f-e9cb-4f5f-86fd-5b8ce74fd715",
   "metadata": {
    "id": "c3645f6f-e9cb-4f5f-86fd-5b8ce74fd715"
   },
   "outputs": [],
   "source": [
    "effnetb2.to(\"cuda\")\n",
    "\n",
    "next(iter(effnetb2.parameters())).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6937edff-50df-4749-98c0-439384e54438",
   "metadata": {
    "id": "6937edff-50df-4749-98c0-439384e54438"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "    \"\"\"Transforms and performs a prediction on img and return prediction and time taken\"\"\"\n",
    "    start_time = timer()\n",
    "    img = effnetb2_transforms(img).unsqueeze(0).to('cuda')\n",
    "\n",
    "    effnetb2.eval()\n",
    "    with torch.inference_mode():\n",
    "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
    "\n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "    pred_time = round(timer() - start_time, 5)\n",
    "\n",
    "    return pred_labels_and_probs, pred_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d719017e-4159-4911-a271-8f55cbca9bd9",
   "metadata": {
    "id": "d719017e-4159-4911-a271-8f55cbca9bd9"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
    "random_image_path = random.sample(test_data_paths, k=1)[0]\n",
    "\n",
    "image = Image.open(random_image_path)\n",
    "print(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")\n",
    "\n",
    "pred_dict, pred_time = predict(img=image)\n",
    "print(f\"Prediction label and probability dictionary: \\n{pred_dict}\\n\")\n",
    "print(f\"Prediciton time: {pred_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OhiQDIODb7Tq",
   "metadata": {
    "id": "OhiQDIODb7Tq"
   },
   "outputs": [],
   "source": [
    "example_list = [[str(filepath) for filepath in random.sample(test_data_paths, k=3)]]\n",
    "example_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-HAsZ9DTePwz",
   "metadata": {
    "id": "-HAsZ9DTePwz"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "title = \"FoodVision Mini ðŸ•ðŸ¥©ðŸ£\"\n",
    "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\n",
    "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
    "\n",
    "demo = gr.Interface(fn=predict,\n",
    "                    inputs=gr.Image(type=\"pil\"),\n",
    "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"),\n",
    "                             gr.Number(label=\"prediction time (s)\")],\n",
    "                    examples=example_list,\n",
    "                    title=title,\n",
    "                    description=description,\n",
    "                    article=article)\n",
    "\n",
    "demo.launch(debug=False,\n",
    "            share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68K9L1w-gCfi",
   "metadata": {
    "id": "68K9L1w-gCfi"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "foodvision_mini_demo_path = Path(\"demos/foodvision_mini/\")\n",
    "\n",
    "if foodvision_mini_demo_path.exists():\n",
    "  shutil.rmtree(foodvision_mini_demo_path)\n",
    "  foodvision_mini_demo_path.mkdir(parent=True,  # Make the parent folders\n",
    "                                  exist_ok=True)  # Create it even if it already exists\n",
    "else:\n",
    "  foodvision_mini_demo_path.mkdir(parents=True,\n",
    "                                  exist_ok=True)\n",
    "\n",
    "!ls demos/foodvision_mini/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4T95UcyRjIr8",
   "metadata": {
    "id": "4T95UcyRjIr8"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "foodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\"\n",
    "foodvision_mini_examples_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "foodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),\n",
    "                            Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),\n",
    "                            Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]\n",
    "\n",
    "# Collect three random test dataset image paths\n",
    "for example in foodvision_mini_examples:\n",
    "  destination = foodvision_mini_examples_path / example.name\n",
    "  print(f\"[INFO] Copying {example} to {destination}\")\n",
    "  shutil.copy2(src=example, dst=destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1akjsBkgkW7l",
   "metadata": {
    "id": "1akjsBkgkW7l"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)]\n",
    "example_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fRShsEB9lV9Y",
   "metadata": {
    "id": "fRShsEB9lV9Y"
   },
   "outputs": [],
   "source": [
    "\n",
    "import shutil\n",
    "\n",
    "effnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n",
    "\n",
    "effnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n",
    "\n",
    "try:\n",
    "    print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")\n",
    "\n",
    "    # Move the model\n",
    "    shutil.move(src=effnetb2_foodvision_mini_model_path,\n",
    "                dst=effnetb2_foodvision_mini_model_destination)\n",
    "\n",
    "    print(f\"[INFO] Model move complete.\")\n",
    "\n",
    "except:\n",
    "  print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved?\")\n",
    "  print(f\"[INFO] Model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aDdgO1gnd33",
   "metadata": {
    "id": "3aDdgO1gnd33"
   },
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_mini/model.py\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "def create_effnetb2_model(num_classes:int=3,\n",
    "                          seed:int=42):\n",
    "  \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
    "\n",
    "  Args:\n",
    "    num_classes (int, optional): number of classes in the classifier head.\n",
    "      Defaults to 3.\n",
    "    seed (int, optional): random seed value. Defaults to 42.\n",
    "  \"\"\"\n",
    "  weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "  transforms = weights.transforms()\n",
    "  model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "  torch.manual_seed(seed)\n",
    "  model.classifier = nn.Sequential(\n",
    "      nn.Dropout(p=0.2, inplace=True),\n",
    "      nn.Linear(in_features=1408, out_features=num_classes),\n",
    "  )\n",
    "  return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eIaXplkYp88u",
   "metadata": {
    "id": "eIaXplkYp88u"
   },
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_mini/app.py\n",
    "import gradio as gr\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from model import create_effnetb2_model\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "class_names = [\"pizza\", \"steak\", \"sushi\"]\n",
    "\n",
    "effnetb2, effnetb2_transforms = create_effnetb2_model(\n",
    "    num_classes=3\n",
    ")\n",
    "\n",
    "effnetb2.load_state_dict(\n",
    "    torch.load(\n",
    "        f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",\n",
    "        map_location=torch.device(\"cpu\")\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "  \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\"\"\"\n",
    "  start_time = timer()\n",
    "  img = effnetb2_transforms(img).unsqueeze(0)\n",
    "\n",
    "  effnetb2.eval()\n",
    "  with torch.inference_mode():\n",
    "    pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
    "\n",
    "  pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "  pred_time = round(timer() - start_time, 5)\n",
    "\n",
    "  return pred_labels_and_probs, pred_time\n",
    "\n",
    "\n",
    "title = \"FoodVision Mini ðŸ•ðŸ¥©ðŸ£\"\n",
    "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\n",
    "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
    "\n",
    "example_list = [[\"examples\" / example] for example in os.listdir(\"examples\")]\n",
    "\n",
    "demo = gr.Interface(fn=predict,\n",
    "                    inputs=gr.Image(type=\"pil\"),\n",
    "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"),\n",
    "                           gr.Number(label=\"Predicton time(s)\")],\n",
    "                    examples=example_list,\n",
    "                    title=title,\n",
    "                    description=description,\n",
    "                    article=article)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OZTHgtC7uOom",
   "metadata": {
    "id": "OZTHgtC7uOom"
   },
   "outputs": [],
   "source": [
    "%%write demos/foodvision_mini/requirements.txt\n",
    "torch>=1.12.0\n",
    "torchvision>=0.13.0\n",
    "gradio>=3.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nckcBiDr7Ac_",
   "metadata": {
    "id": "nckcBiDr7Ac_"
   },
   "outputs": [],
   "source": [
    "!ls demos/foodvision_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XI9bciNB7Udr",
   "metadata": {
    "id": "XI9bciNB7Udr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/KajetanFrackowiak/pytorch_deep_learning/blob/main/09.pytorch_model_deployment.ipynb",
     "timestamp": 1712865534854
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d57e5693e2340b19dd6835f3a14625f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2e8e3b8a7ef2449ba6f2d18d257c0ffb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "326523d6008940a4876c4fb33fcf5a24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40fb9a6bf6f94c5fbc2234c3455f7d60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_326523d6008940a4876c4fb33fcf5a24",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0d57e5693e2340b19dd6835f3a14625f",
      "value": "â€‡5/10â€‡[14:07&lt;13:47,â€‡165.59s/it]"
     }
    },
    "50ae02cf5a364174919949e4334033bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d779ab7f66646f9b096b2ee3d69a540": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50ae02cf5a364174919949e4334033bd",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2e8e3b8a7ef2449ba6f2d18d257c0ffb",
      "value": 5
     }
    },
    "a77174417993483e884f233ab66e08a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b30a67d3802d475b83eec5347bb5d7df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0fc3957ebc0450ca3b3ea0a8d5e3441": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fdecda765a884f39a3829b82cca3709e",
       "IPY_MODEL_6d779ab7f66646f9b096b2ee3d69a540",
       "IPY_MODEL_40fb9a6bf6f94c5fbc2234c3455f7d60"
      ],
      "layout": "IPY_MODEL_f478c6f185544f41a40d600b6274a06b"
     }
    },
    "f478c6f185544f41a40d600b6274a06b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fdecda765a884f39a3829b82cca3709e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b30a67d3802d475b83eec5347bb5d7df",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a77174417993483e884f233ab66e08a6",
      "value": "â€‡50%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
